{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1a5ac58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "\n",
    "from lib.KDTreeEncoding import *\n",
    "\n",
    "import xgboost as xgb\n",
    "from lib.XGBHelper import *\n",
    "from lib.XGBoost_params import *\n",
    "from lib.score_analysis import *\n",
    "\n",
    "from lib.logger import logger\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy import load\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9233806d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up random seed, it has been set in all helper lib too\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b100edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set path to directories here\n",
    "poverty_dir = '/datasets/cs255-sp22-a00-public/poverty/'\n",
    "#poverty_dir = '/home/tlaud/climate_glue/cse255'\n",
    "image_dir = poverty_dir + '/anon_images'\n",
    "train_table = '../public_tables/train.csv'\n",
    "#train_table = '/home/tlaud/climate_glue/cse255/public_tables/train.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fc4b3bd",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../public_tables/train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-d5b7d1021e67>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# creating main dataframe and image files here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_table\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'filename'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{image_dir}/*.npz'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../public_tables/train.csv'"
     ]
    }
   ],
   "source": [
    "# creating main dataframe and image files here\n",
    "df=pd.read_csv(train_table,index_col=0)\n",
    "df.index = df['filename']\n",
    "files=list(glob(f'{image_dir}/*.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15875577",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Pickled the encoded dataset, so can skip this cell if no changes are being made here (pickle again if Yes)\n",
    "## Getting features from the images in the main dataframe Enc_data using KD-trees\n",
    "## Can think of changes here to extract features from the images in a different manner\n",
    "## max_images = 610 works, but 615 will run out of memory and kernel dies\n",
    "## try changing the randomness factor here file choosing images to use for encoding\n",
    "## tree depth: (2^(tree_depth+1))+1 will decide how many columns we will be having in our encoded dataset\n",
    "## so right now 1024+1 columns will be used to encode the image data as a feature vector\n",
    "tree_depth = 9\n",
    "train_size,tree=train_encoder(files,max_images=610,tree_depth=tree_depth)\n",
    "Enc_data=encoded_dataset(image_dir,df,tree, depth=tree_depth, label_col='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9cfe11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving this encoded dataset and tree so that we don't have to encode it every time\n",
    "# run this cell only if you have made changes to the encoding design\n",
    "encoded_dataset=f'encoded_dataset.pk'\n",
    "encoder_tree=f'encoder_tree.pk'\n",
    "pkl.dump(Enc_data,open(encoded_dataset,'wb'))\n",
    "pkl.dump(tree, open(encoder_tree,'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "406f1034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell to load the encoded dataset and tree \n",
    "encoded_dataset=f'encoded_dataset.pk'\n",
    "encoder_tree = f'encoder_tree.pk'\n",
    "Enc_data = pkl.load(open(encoded_dataset,'rb'))\n",
    "tree = pkl.load(open(encoder_tree,'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c82b591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating object D to handle dataset functionalities\n",
    "# like getting subsets, bootstrapping samples, etc\n",
    "D = DataSplitter(Enc_data.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "79a9fcc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 3,\n",
       " 'eta': 0.18,\n",
       " 'verbosity': 0,\n",
       " 'objective': 'binary:logistic',\n",
       " 'nthread': 7,\n",
       " 'eval_metric': ['auc'],\n",
       " 'verbose_eval': 1,\n",
       " 'early_stopping_rounds': 10,\n",
       " 'feature_selector': 'shuffle',\n",
       " 'custom_metric': <function __main__.calc_f1(predt: numpy.ndarray, dtrain: xgboost.core.DMatrix)>,\n",
       " 'disable_default_eval_metric': True,\n",
       " 'num_round': 200}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, recall_score\n",
    "# Set all params for XGBoost here\n",
    "# param dictionary is already present in the lib.XGBoost_Params file\n",
    "# default parameters are set, can be customized and new params can also be added\n",
    "\n",
    "def calc_f1(predt: np.ndarray, dtrain: xgb.DMatrix):\n",
    "    preds = (predt > 0.5).astype(np.int64)\n",
    "    res = f1_score(predt>0.5, dtrain.get_label())\n",
    "    return 'f1', res\n",
    "\n",
    "param['max_depth'] = 3   # depth of tree\n",
    "param['eta'] = 0.18      # shrinkage parameter\n",
    "#COPY THIS\n",
    "param['verbose_eval'] = 1 \n",
    "param['early_stopping_rounds'] = 10\n",
    "param['eval_metric'] = ['auc']\n",
    "param['feature_selector'] = 'shuffle'\n",
    "param['custom_metric'] = calc_f1\n",
    "param['disable_default_eval_metric']=True\n",
    "# param['objective'] = 'binary:logistic'\n",
    "# param['nthread'] = 7 # Number of threads used\n",
    "# param['eval_metric'] = ['error','logloss']\n",
    "param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c0133c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training subset has:  7387  samples\n",
      "Testing subset has:  3978  samples\n"
     ]
    }
   ],
   "source": [
    "# deciding which rows in dataset to select [True, False, False, True,.....]\n",
    "train_selector=np.random.rand(df.shape[0]) > 0.35\n",
    "# subset selecting everything which is True as Train set\n",
    "Train=D.get_subset(train_selector)\n",
    "# subset selecting everything which is False as Test set\n",
    "Test=D.get_subset(~train_selector)\n",
    "\n",
    "# checking the size of the train and test dataset train should be more\n",
    "print(\"Training subset has: \", Train.shape[0], \" samples\")\n",
    "print(\"Testing subset has: \", Test.shape[0], \" samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52723475",
   "metadata": {},
   "outputs": [],
   "source": [
    "param['num_round']=200\n",
    "log200=simple_bootstrap('xgb',Train,Test,param,ensemble_size=10)\n",
    "\n",
    "styled_logs=[    \n",
    "    {   'log':log200,\n",
    "        'style':['r:','r-'],\n",
    "        'label':'200 iterations',\n",
    "        'label_color':'r'\n",
    "    }\n",
    "]\n",
    "_mean,_std=plot_scores(styled_logs,title='All')\n",
    "\n",
    "pickle_file=f'data/Checkpoint.pk'\n",
    "Dump={'styled_logs':styled_logs,\n",
    "     'tree':tree,\n",
    "     'mean':_mean,\n",
    "     'std':_std}\n",
    "pkl.dump(Dump,open(pickle_file,'wb'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98de4b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only for Urban Areas now\n",
    "urban=True\n",
    "area= 'Urban' if urban else 'Rural'\n",
    "selector=df['urban']==urban\n",
    "subData=D.get_subset(selector)\n",
    "subD=DataSplitter(subData)\n",
    "\n",
    "train_selector=np.random.rand(subData.shape[0]) > 0.7\n",
    "Train=subD.get_subset(train_selector)\n",
    "Test=subD.get_subset(~train_selector)\n",
    "\n",
    "param['num_round']=10\n",
    "log10=simple_bootstrap(Train,Test,param,ensemble_size=30)\n",
    "param['num_round']=100\n",
    "log100=simple_bootstrap(Train,Test,param,ensemble_size=30)\n",
    "\n",
    "styled_logs=[\n",
    "    {   'log':log10,\n",
    "        'style':['g:','g-'],\n",
    "        'label':'10 iterations',\n",
    "        'label_color':'g'\n",
    "    },\n",
    "    {   'log':log100,\n",
    "        'style':['b:','b-'],\n",
    "        'label':'100 iterations',\n",
    "        'label_color':'b'\n",
    "    }\n",
    "]\n",
    "\n",
    "_mean,_std=plot_scores(styled_logs,title=f'{area}Only: Split into train and test at random')\n",
    "\n",
    "pickle_file=f'data/Dump{area}.pk'\n",
    "Dump={'styled_logs':styled_logs,\n",
    "     'tree':tree,\n",
    "     'mean':_mean,\n",
    "     'std':_std}\n",
    "pkl.dump(Dump,open(pickle_file,'wb'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed038101",
   "metadata": {},
   "outputs": [],
   "source": [
    "_mean, _std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7ca33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "urban=False\n",
    "area= 'Urban' if urban else 'Rural'\n",
    "selector=df['urban']==urban\n",
    "subData=D.get_subset(selector)\n",
    "subD=DataSplitter(subData)\n",
    "\n",
    "train_selector=np.random.rand(subData.shape[0]) > 0.7\n",
    "Train=subD.get_subset(train_selector)\n",
    "Test=subD.get_subset(~train_selector)\n",
    "\n",
    "param['num_round']=10\n",
    "log10=simple_bootstrap(Train,Test,param,ensemble_size=30)\n",
    "param['num_round']=100\n",
    "log100=simple_bootstrap(Train,Test,param,ensemble_size=30)\n",
    "\n",
    "styled_logs=[\n",
    "    {   'log':log10,\n",
    "        'style':['y:','y-'],\n",
    "        'label':'10 iterations',\n",
    "        'label_color':'y'\n",
    "    },\n",
    "    {   'log':log100,\n",
    "        'style':['m:','m-'],\n",
    "        'label':'100 iterations',\n",
    "        'label_color':'m'\n",
    "    }\n",
    "]\n",
    "\n",
    "_mean,_std=plot_scores(styled_logs,title=f'{area}Only: Split into train and test at random')\n",
    "\n",
    "pickle_file=f'data/Dump{area}.pk'\n",
    "Dump={'styled_logs':styled_logs,\n",
    "     'tree':tree,\n",
    "     'mean':_mean,\n",
    "     'std':_std}\n",
    "pkl.dump(Dump,open(pickle_file,'wb'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5816ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a18efc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
